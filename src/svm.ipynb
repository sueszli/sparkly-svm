{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, IDF, StringIndexer, ChiSqSelector, CountVectorizer, CountVectorizerModel, Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col\n",
    "import pathlib\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql.functions import size\n",
    "from pyspark.ml.param import Param, Params\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "base_path = pathlib.Path(\"Exercise_2/sparkly-svm/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a logger to track some of the stuff\n",
    "logging.basicConfig(filename=base_path.parent/'data/svm_training.log', level=logging.INFO, \n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Setting up the Spark Session in cluster mode, managed by yarn. \n",
    "    5 Executors with each 4 cores and 4GB of RAM. \n",
    "    \"\"\"\n",
    "    conf = SparkConf() \\\n",
    "    .setAppName(\"svm\") \\\n",
    "    .setMaster(\"yarn\") \\\n",
    "    .set(\"spark.executor.memory\", \"4g\") \\\n",
    "    .set(\"spark.driver.memory\", \"4g\") \\\n",
    "    .set(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .set(\"spark.executor.instances\", \"5\") \\\n",
    "    .set(\"spark.executor.cores\", \"4\") \\\n",
    "    .set(\"spark.default.parallelism\", \"20\")\n",
    "\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "STOPWORD_PATH = base_path.parent/\"data/stopwords.txt\"\n",
    "\n",
    "spark = create_spark_session()\n",
    "\n",
    "with open(STOPWORD_PATH, 'r') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "df = spark.read.json(str(DATA_PATH)).select(\"reviewText\", \"category\")\n",
    "regex = r'[ \\t\\d()\\[\\]{}.!?,;:+=\\-_\"\\'~#@&*%€$§\\/]+'\n",
    "\n",
    "logging.info(f\"Stopwords loaded from: {STOPWORD_PATH}\")\n",
    "logging.info(f\"Data loaded from hdfs: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing pipeline serves to prepare the data for model fitting. This modular approach allows us to decouple data preprocessing from model hyperparameter tuning and fitting, enhancing code clarity and maintainability. To eliminate any overhead generated during preprocessing and to materialize any pending lazy operations, we save the data and restart the Spark session.\n",
    "\n",
    "**Important:** To ensure reproducibility, it is crucial to cache or persist the DataFrame prior to splitting the data into training, validation, and test sets. Failing to do so can result in non-deterministic behavior from PySpark's `randomSplit()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Preprocessing started.\")\n",
    "preprocess_pipeline = Pipeline(stages=[\n",
    "    RegexTokenizer(inputCol=\"reviewText\", outputCol=\"rawTerms\", pattern=regex),\n",
    "    StopWordsRemover(inputCol=\"rawTerms\", outputCol=\"terms\", stopWords=stopwords),\n",
    "    StringIndexer(inputCol=\"category\", outputCol=\"label\"),\n",
    "    CountVectorizer(inputCol=\"terms\", outputCol=\"rawFeatures\"),\n",
    "    IDF(inputCol=\"rawFeatures\", outputCol=\"features\"),\n",
    "])\n",
    "\n",
    "preprocessor = preprocess_pipeline.fit(df)\n",
    "df_prep = preprocessor.transform(df)\n",
    "logging.info(\"Preprocessing finished.\")\n",
    "\n",
    "df_prep = df_prep.filter(size(df_prep.terms) > 0)\n",
    "df_prep = df_prep.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "train, val, test = df_prep.randomSplit([0.6, 0.2, 0.2], seed=1)\n",
    "train.write.mode('overwrite').parquet(\"train.parquet\")\n",
    "val.write.mode('overwrite').parquet(\"val.parquet\")\n",
    "test.write.mode('overwrite').parquet(\"test.parquet\")\n",
    "\n",
    "preprocessor.write().overwrite().save(str(base_path/\"preprocessor\"))\n",
    "logging.info(\"Preprocessor pipeline model saved.\")\n",
    "logging.info(\"Preprocessed data saved.\")\n",
    "spark.stop()\n",
    "spark = create_spark_session()\n",
    "train = spark.read.parquet(\"train.parquet\")\n",
    "val = spark.read.parquet(\"val.parquet\")\n",
    "test = spark.read.parquet(\"test.parquet\")\n",
    "train.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "logging.info(\"Preprocessed data loaded with new SparkSession.\")\n",
    "logging.info(f\"Training size: {train.count()}\")\n",
    "logging.info(f\"Validation size: {val.count()}\")\n",
    "logging.info(f\"Testing size: {test.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the ample amount of data available for training, validation, and testing, we opt not to employ K-Fold CrossValidation. This decision is motivated by the computational intensity of K-Fold CrossValidation, which is unnecessary given our substantial sample size. Validation on a separate set is sufficient for our needs. Consequently, we manually conduct a grid search to explore various hyperparameter settings. In total we obtail 24 different combinations of hyperparameter settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = datetime.now()\n",
    "hyperparameter_configs = {\n",
    "    \"numTopFeatures\": [10, 2000], \n",
    "    \"regParam\": [0.01, 0.1, 1], \n",
    "    \"maxIter\": [10, 100], \n",
    "    \"p\": [1, 2]\n",
    "}\n",
    "\n",
    "keys, values = zip(*hyperparameter_configs.items())\n",
    "hyperparameter_configs_list = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c70ef17b22f473b8d1d5a6268c32015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hyperparameters:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 44390)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 44404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 44418)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 53518)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 53526)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 53540)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52112)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52114)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52118)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51854)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51870)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51884)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 59188)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 59198)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 59202)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50962)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 39576)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 39584)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 39596)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 42236)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 42250)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 42258)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'numTopFeatures': 10, 'regParam': 0.01, 'maxIter': 10, 'p': 1, 'f1_train': 0.8727422906218616, 'f1_val': 0.6018395632466248, 'runtime_training': 96, 'runtime_train_pred': 4, 'runtime_train_val': 3, 'runtime_train_eval': 10}\n",
      "{'numTopFeatures': 10, 'regParam': 0.01, 'maxIter': 10, 'p': 2, 'f1_train': 0.8727422906218616, 'f1_val': 0.6018395632466248, 'runtime_training': 80, 'runtime_train_pred': 3, 'runtime_train_val': 3, 'runtime_train_eval': 5}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m t0 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     24\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining started with configs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyperparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m fitted_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_training\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m t1 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     28\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished. Runtime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;241m-\u001b[39mt0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/pipeline.py:114\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/classification.py:2924\u001b[0m, in \u001b[0;36mOneVsRest._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   2920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m classifier\u001b[38;5;241m.\u001b[39mfit(trainingDataset, paramMap)\n\u001b[1;32m   2922\u001b[0m pool \u001b[38;5;241m=\u001b[39m ThreadPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetParallelism(), numClasses))\n\u001b[0;32m-> 2924\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43minheritable_thread_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainSingleClass\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnumClasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handlePersistence:\n\u001b[1;32m   2927\u001b[0m     multiclassLabeled\u001b[38;5;241m.\u001b[39munpersist()\n",
      "File \u001b[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib64/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_best_model = 0\n",
    "training_results = []\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "for hyperparam in tqdm(hyperparameter_configs_list, desc='Hyperparameters'): \n",
    "    pipeline_training = Pipeline(stages=[\n",
    "        ChiSqSelector(\n",
    "            featuresCol=\"features\", \n",
    "            outputCol=\"selectedFeatures\", \n",
    "            labelCol=\"label\", \n",
    "            numTopFeatures=hyperparam['numTopFeatures']),\n",
    "        # l2/l1-normalize vector length\n",
    "        Normalizer(\n",
    "            inputCol=\"selectedFeatures\", \n",
    "            outputCol=\"normalizedFeatures\", \n",
    "            p = hyperparam['p']),\n",
    "        # train SVM\n",
    "        OneVsRest(classifier=LinearSVC(\n",
    "            featuresCol=\"normalizedFeatures\", \n",
    "            labelCol=\"label\", \n",
    "            regParam=hyperparam['regParam'], \n",
    "            maxIter=hyperparam['maxIter']))])\n",
    "    \n",
    "    t0 = datetime.now()\n",
    "    logging.info(f\"Training started with configs {hyperparam}.\")\n",
    "    \n",
    "    fitted_model = pipeline_training.fit(train)\n",
    "    t1 = datetime.now()\n",
    "    logging.info(f\"Training finished. Runtime: {t1-t0}\")\n",
    "    \n",
    "    predictions_train = fitted_model.transform(train)\n",
    "    t2 = datetime.now()\n",
    "    logging.info(f\"Train prediction runtime: {t2-t1}\")\n",
    "\n",
    "    predictions_val = fitted_model.transform(val)\n",
    "    t3 = datetime.now()\n",
    "    logging.info(f\"Val prediction runtime: {t3-t2}\")\n",
    "    \n",
    "    f1_train = evaluator.evaluate(predictions_train)\n",
    "    t4 = datetime.now()\n",
    "    logging.info(f\"Train evaluation runtime: {t4-t3}, F1: {f1_train}\")\n",
    "\n",
    "    f1_val = evaluator.evaluate(predictions_val)\n",
    "    t5 = datetime.now()\n",
    "    logging.info(f\"Val evaluation runtime: {t5-t4}, F1: {f1_val}\")\n",
    "    \n",
    "    if f1_val > current_best_model:\n",
    "        logging.info(f\"New best hyperparameter combinatioon found with validation F1: {f1_val} and configs: {hyperparam}.\")\n",
    "        fitted_model.write().overwrite().save(str(base_path.parent/\"data/best_svm_model\"))\n",
    "        logging.info(f\"Model saved.\")\n",
    "    \n",
    "    train_run_results = {\n",
    "        \"numTopFeatures\": hyperparam[\"numTopFeatures\"], \n",
    "        \"regParam\": hyperparam[\"regParam\"], \n",
    "        \"maxIter\": hyperparam[\"maxIter\"], \n",
    "        \"p\": hyperparam[\"p\"],\n",
    "        \"f1_train\": f1_train, \n",
    "        \"f1_val\": f1_val,\n",
    "        \"runtime_training\": int((t1 - t0).total_seconds()),\n",
    "        \"runtime_train_pred\": int((t2 - t1).total_seconds()), \n",
    "        \"runtime_train_val\": int((t3 - t2).total_seconds()), \n",
    "        \"runtime_train_eval\": int((t4 - t3).total_seconds()), \n",
    "        \"runtime_train_eval\": int((t5 - t4).total_seconds())}\n",
    "    \n",
    "    train_run_results_df = pd.DataFrame([train_run_results])\n",
    "    training_results.append(train_run_results)\n",
    "    if len(training_results) == 1:\n",
    "        train_run_results_df.to_csv(str(base_path.parent/\"data/model_training_metrics.csv\"), mode='a', index=False, header=True)\n",
    "    else:\n",
    "        train_run_results_df.to_csv(str(base_path.parent/\"data/model_training_metrics.csv\"), mode='a', index=False, header=False)\\\n",
    "    \n",
    "    print(train_run_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = PipelineModel.load(str(base_path.parent/\"data/best_svm_model\"))\n",
    "t0 = datetime.now()\n",
    "predictions_test = loaded_model.transform(test) \n",
    "t1 = datetime.now()\n",
    "f1_test = evaluator.evaluate(predictions_val)\n",
    "t2 = datetime.now()\n",
    "logging.info(f\"Runtime prediciton: {t1-t0}, runtime evalutation {t2-t1}, F1 Test: {f1_test}\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC24)",
   "language": "python",
   "name": "python3_dic24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
